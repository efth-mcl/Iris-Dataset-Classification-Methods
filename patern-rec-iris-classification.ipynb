{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pattern Recognition-Iris Dataset Classification\n",
    "**IRIS DATASET 150 records**\n",
    "Every record has 4 features of datatype float and 1 true class index of datatype integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern Recognition\n",
    "# Classification of Iris Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "# Load Iris\n",
    "class Irisdataset:\n",
    "    feature_names = ['Sepal length', 'Sepal width', 'Petal length', 'Petal width']\n",
    "    set_names = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\n",
    "    \n",
    "    # trn: number of traning examples\n",
    "    # 150-trn: number of test examples\n",
    "    def __init__(self, trn=120):\n",
    "        # 0<=TR_data<=150\n",
    "        N = trn\n",
    "        # Testing samples 150-N\n",
    "        self._AR = np.zeros((150, 5))\n",
    "        \n",
    "        if not os.path.isfile('iris.data'):\n",
    "            print('Download IRIS dataset')\n",
    "            urllib.request.urlretrieve(\n",
    "                \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\",\n",
    "                \"iris.data\"\n",
    "            )\n",
    "            print('Download complete')\n",
    "\n",
    "        with open('iris.data', 'r') as f:\n",
    "            i = 0\n",
    "            for Line in f:\n",
    "                line = Line[:-1].split(',')\n",
    "                line[0] = float(line[0])\n",
    "                line[1] = float(line[1])\n",
    "                line[2] = float(line[2])\n",
    "                line[3] = float(line[3])\n",
    "                if(line[4] == 'Iris-setosa'):\n",
    "                    line[4] = 0.0\n",
    "                elif(line[4] == 'Iris-versicolor'):\n",
    "                    line[4] = 1.0\n",
    "                elif(line[4] == 'Iris-virginica'):\n",
    "                    line[4] = 2.0\n",
    "\n",
    "                self._AR[i] = np.array(line)\n",
    "                i += 1\n",
    "                if i == 150:\n",
    "                    break\n",
    "        self._AR = np.random.permutation(self._AR) # random sort\n",
    "        self.TrainData = self._AR[:N]\n",
    "        self.TestData = self._AR[N:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Classification - Perceptron\n",
    "One Layer Perceptron with 3 simple neurons,<br>\n",
    "choice two features for classifier inputs,<br>\n",
    "Training set: 120 examples<br>\n",
    "Test set:     30 examples<br>\n",
    "Final classification error average errors choosing training & test examples 5 times at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methods\n",
    "Hot = lambda x:[1 if i==x else -1 for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.7\n",
    "total_test_error = 0\n",
    "\n",
    "f0 = 3\n",
    "f1 = 1\n",
    "for cnt in range(5):\n",
    "    iris = Irisdataset(120)  # 120 train data\n",
    "    train_data = iris.TrainData[:,[f0,f1]]\n",
    "    train_hot = -1*np.ones((120,3))\n",
    "    train_hot[range(120),iris.TrainData[:,-1].astype(int)] = 1\n",
    "    \n",
    "    test_data = iris.TestData[:,[f0,f1]]\n",
    "    test_hot = -1*np.ones((30,3))\n",
    "    test_hot[range(30),iris.TestData[:,-1].astype(int)] = 1\n",
    "    \n",
    "    W = np.random.rand(2, 3) # Weights\n",
    "    B = np.random.rand(1, 3) # Bias\n",
    "    \n",
    "    # Training\n",
    "    # update W, B 200 times or Epochs = 200\n",
    "    for i in range(200): \n",
    "        tr_error = 0\n",
    "        for tr_d,tr_hot in zip(train_data,train_hot):\n",
    "            # forward pass\n",
    "            tr_d = tr_d.reshape(1, 2)\n",
    "            neurons_outs = np.dot(tr_d, W)+B\n",
    "            \n",
    "            # outputs indexis with wrong prediction class\n",
    "            tr_error+=np.where(neurons_outs*tr_hot<0)[0].size>0\n",
    "            \n",
    "            # backward pass\n",
    "            neurons_outs[np.where(neurons_outs*tr_hot>0)] = 0\n",
    "            neurons_outs = np.sign(neurons_outs)\n",
    "            \n",
    "            # Deltas\n",
    "            DW = -learning_rate*np.dot(tr_d.T,neurons_outs)\n",
    "            DB = -learning_rate*neurons_outs\n",
    "            \n",
    "            # Update\n",
    "            W += DW\n",
    "            B += DB\n",
    "            \n",
    "    test_neurons_outs = np.dot(test_data, W)+B\n",
    "    test_prediction = test_neurons_outs.argmax(1)\n",
    "    ts_error = sum(test_hot.argmax(1) != test_prediction)\n",
    "    total_test_error += ts_error\n",
    "    \n",
    "    print('Round {0:d}: Test error={1:.2f}'.format(cnt+1, ts_error/30))\n",
    "\n",
    "print('\\nAverage Test error={0:.2f}'.format(total_test_error/(5*30)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True labels\n",
    "c1_tr_train = train_data[np.where(iris.TrainData[:,4] == 0)]\n",
    "c2_tr_train = train_data[np.where(iris.TrainData[:,4] == 1)]\n",
    "c3_tr_train = train_data[np.where(iris.TrainData[:,4] == 2)]\n",
    "\n",
    "c1_tr_test = test_data[np.where(iris.TestData[:,4] == 0)]\n",
    "c2_tr_test = test_data[np.where(iris.TestData[:,4] == 1)]\n",
    "c3_tr_test = test_data[np.where(iris.TestData[:,4] == 2)]\n",
    "\n",
    "# Predicted labels for test examples\n",
    "c1_pr_test = test_data[test_prediction == 0]\n",
    "c2_pr_test = test_data[test_prediction == 1]\n",
    "c3_pr_test = test_data[test_prediction == 2]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8), dpi= 80, facecolor='w', edgecolor='k' )\n",
    "fig.subplots_adjust(\n",
    "    left  = 0,\n",
    "    right = 1,\n",
    "    wspace=0.4,\n",
    "    hspace = 0.2,\n",
    "    top=1\n",
    ")\n",
    "\n",
    "c1_c = 'royalblue'\n",
    "c2_c = 'darkorange'\n",
    "c3_c = 'limegreen'\n",
    "\n",
    "ax.plot(c1_tr_train[:,0],c1_tr_train[:,1],'o',color=c1_c)\n",
    "ax.plot(c2_tr_train[:,0],c2_tr_train[:,1],'o',color=c2_c)\n",
    "ax.plot(c3_tr_train[:,0],c3_tr_train[:,1],'o',color=c3_c)\n",
    "\n",
    "ms_c1 = dict(color=c1_c, marker='o')\n",
    "ms_c2 = dict(color=c2_c, marker='o')\n",
    "ms_c3 = dict(color=c3_c, marker='o')\n",
    "\n",
    "ax.plot(c1_tr_test[:,0],c1_tr_test[:,1],'o', **ms_c1, markersize=15, fillstyle='none')\n",
    "ax.plot(c2_tr_test[:,0],c2_tr_test[:,1],'o', **ms_c2, markersize=15, fillstyle='none')\n",
    "ax.plot(c3_tr_test[:,0],c3_tr_test[:,1],'o', **ms_c3, markersize=15, fillstyle='none')\n",
    "\n",
    "\n",
    "ax.plot(c1_pr_test[:,0],c1_pr_test[:,1],'o', **ms_c1)\n",
    "ax.plot(c2_pr_test[:,0],c2_pr_test[:,1],'o', **ms_c2)\n",
    "ax.plot(c3_pr_test[:,0],c3_pr_test[:,1],'o', **ms_c3)\n",
    "\n",
    "ax.set_xlabel(iris.feature_names[f0], fontsize=20)\n",
    "ax.set_ylabel(iris.feature_names[f1], fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Square Error(MSE), leave-one-out cross validation error\n",
    "\n",
    "Minimize MSE of a linear classifier using normal equation.\n",
    "Using all features for train & validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methods\n",
    "def normal_eq(X,Y):\n",
    "    W = np.linalg.inv(np.dot(X.T, X))\n",
    "    W = np.dot(np.dot(W, X.T), Y)\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = Irisdataset(150)\n",
    "\n",
    "def_tr_index = lambda x,n:[False if j==x else True for j in range(n)]\n",
    "error = 0\n",
    "S_Examples = iris.TrainData\n",
    "N = S_Examples.shape[0]\n",
    "\n",
    "S_Data = S_Examples[:,:-1]\n",
    "S_Data = np.concatenate((np.ones((N,1)),S_Data),axis=1)\n",
    "S_Hot = np.zeros((N,3))\n",
    "S_Hot[range(N),S_Examples[:,-1].astype(int)] = 1\n",
    "for i in range(N):\n",
    "    tr_index = np.array(def_tr_index(i,N))\n",
    "\n",
    "    tr_data = S_Data[tr_index]\n",
    "    tr_hot = np.array([S_Hot[tr_index] == 1]).reshape(-1,3)\n",
    "    \n",
    "    ts_data = S_Data[i].reshape(1,-1)\n",
    "    ts_hot = np.array([S_Hot[i] == 1]).reshape(-1,3)\n",
    "    W = normal_eq(tr_data,tr_hot)\n",
    "    W[0,:]+=0\n",
    "    ts_pr = np.dot(ts_data,W)\n",
    "    cl_p = np.sum(ts_pr.argmax()!=ts_hot.argmax())\n",
    "    error += cl_p\n",
    "    \n",
    "print('Test error: {0:.2f}'.format(error/N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f0 = 3 # first feature \n",
    "f1 = 1 # second feature\n",
    "f1+=1\n",
    "f0+=1\n",
    "c1_tr = S_Data[np.where(S_Hot.argmax(1) == 0)]\n",
    "c2_tr = S_Data[np.where(S_Hot.argmax(1) == 1)]\n",
    "c3_tr = S_Data[np.where(S_Hot.argmax(1) == 2)]\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(8, 4), dpi= 80, facecolor='w', edgecolor='k' )\n",
    "fig.subplots_adjust(\n",
    "    left  = 0,\n",
    "    right = 1,\n",
    "    wspace=0.4,\n",
    "    hspace = 0.2,\n",
    "    top=1\n",
    ")\n",
    "\n",
    "ax[0].plot(\n",
    "        c1_tr[:,f0],c1_tr[:,f1],'o',\n",
    "        c2_tr[:,f0],c2_tr[:,f1],'o',\n",
    "        c3_tr[:,f0],c3_tr[:,f1],'o'\n",
    ")\n",
    "\n",
    "ax[0].set_title('true labels')\n",
    "ax[0].set_xlabel(iris.feature_names[f0-1])\n",
    "ax[0].set_ylabel(iris.feature_names[f1-1])\n",
    "\n",
    "pr_d = S_Data.dot(W) \n",
    "pl1 = S_Data[np.where(pr_d.argmax(1)== 0)[0]][:,[f0,f1]]\n",
    "pl2 = S_Data[np.where(pr_d.argmax(1) == 1)[0]][:,[f0,f1]]\n",
    "pl3 = S_Data[np.where(pr_d.argmax(1) == 2)[0]][:,[f0,f1]]\n",
    "\n",
    "sc1, sc2, sc3 = ax[1].plot(\n",
    "                        pl1[:,0],pl1[:,1],'o',\n",
    "                        pl2[:,0],pl2[:,1],'o',\n",
    "                        pl3[:,0],pl3[:,1],'o'\n",
    "                    )\n",
    "ax[1].set_title('predicted labels')\n",
    "ax[1].set_xlabel(iris.feature_names[f0-1])\n",
    "ax[1].set_ylabel(iris.feature_names[f1-1])\n",
    "\n",
    "fig.legend((sc1, sc2, sc3),\n",
    "           tuple(iris.set_names),\n",
    "           'upper center',\n",
    "           fontsize=16,\n",
    "           title='classes',\n",
    "           bbox_to_anchor=[0.55, 1.5]\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes classifier, leave-one-out cross validation\n",
    "* hypothesis 1: all features are independent and following the normal distribution \n",
    "* hypothesis 2: all fatures follow 4D normal distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methods\n",
    "Normal_dist = lambda x,avg, std: 1/np.sqrt(2*np.pi*std**2)*np.exp(-(x-avg)**2/(2*std**2))\n",
    "\n",
    "ND_mahalanobis = lambda x,mean,S: np.dot(np.dot((x-mean).T,np.linalg.inv(S)),(x-mean))**(1/2)\n",
    "\n",
    "def ND_normal_dist(x,mean,S):\n",
    "    m_dist = ND_mahalanobis(x, mean, S)\n",
    "    constant = 1/((2*np.pi)**(4/2)*np.linalg.det(S)**(1/2))\n",
    "    return constant*np.exp(-1/2*np.square(m_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = Irisdataset(150) # all dataset\n",
    "\n",
    "error_h1=0 # hypothesis_1 error\n",
    "error_h2=0 # hypothesis_2 error\n",
    "\n",
    "def_tr_index = lambda x:[False if j==x else True for j in range(150)]\n",
    "for i in range(150):\n",
    "    \n",
    "    tr_index = np.array(def_tr_index(i))\n",
    "    tr_data = iris.TrainData[tr_index]\n",
    "    # get i_th record for validation\n",
    "    vl_data = iris.TrainData[i]\n",
    "    \n",
    "    \n",
    "    # Estimate maximum likelihood per feature\n",
    "    # AVG - STD\n",
    "    # Setosa\n",
    "    tr_setosa = tr_data[np.where(tr_data[:,4] == 0)][:,:4]\n",
    "    tr_setosa_avg_per_feature = np.mean(tr_setosa,axis=0)\n",
    "    tr_setosa_std_per_feature = np.std(tr_setosa, axis=0)\n",
    "\n",
    "    # Versicolor\n",
    "    tr_versicolor = tr_data[np.where(tr_data[:,4] == 1)][:,:4]\n",
    "    tr_versicolor_avg_per_feature = np.mean(tr_versicolor, axis=0)\n",
    "    tr_versicolor_std_per_feature = np.std(tr_versicolor, axis=0)\n",
    "\n",
    "    # Virginica\n",
    "    tr_virginica = tr_data[np.where(tr_data[:,4] == 2)][:,:4]\n",
    "    tr_virginica_avg_per_feature = np.mean(tr_virginica, axis=0)\n",
    "    tr_virginica_std_per_feature = np.std(tr_virginica, axis=0)\n",
    "    \n",
    "    \n",
    "    # hypothesis 1 START\n",
    "    # Test Model\n",
    "    # class confidence per feature\n",
    "    h1_conf_setosa = Normal_dist(vl_data[:-1], tr_setosa_avg_per_feature, tr_setosa_std_per_feature)\n",
    "    h1_conf_versicolor = Normal_dist(vl_data[:-1], tr_versicolor_avg_per_feature, tr_versicolor_std_per_feature)\n",
    "    h1_conf_virginica = Normal_dist(vl_data[:-1], tr_virginica_avg_per_feature, tr_virginica_std_per_feature)\n",
    "    \n",
    "    # prediction_class\n",
    "    arg_max_class_conf_per_feature = np.argmax([\n",
    "            h1_conf_setosa,\n",
    "            h1_conf_versicolor,\n",
    "            h1_conf_virginica      \n",
    "        ], axis=0)\n",
    "    counts = np.bincount(arg_max_class_conf_per_feature)\n",
    "    h1_prediction_class = np.argmax(counts)\n",
    "    \n",
    "    # add 1 if true_class != prediction_class\n",
    "    error_h1 += h1_prediction_class != vl_data[-1]\n",
    "    # hypothesis 1 END\n",
    "    \n",
    "    # hypothesis 2 START\n",
    "    # calculate converance by class\n",
    "    tr_setosa_cov = np.cov((tr_setosa).T)\n",
    "    tr_versicolor_cov = np.cov((tr_versicolor).T)\n",
    "    tr_virginica_cov = np.cov((tr_virginica).T)\n",
    "    \n",
    "    # class confidence \n",
    "    h2_conf_setosa = ND_normal_dist(vl_data[:-1].reshape(4,1),tr_setosa_avg_per_feature.reshape(4,1),tr_setosa_cov)\n",
    "    h2_conf_versicolor = ND_normal_dist(vl_data[:-1].reshape(4,1),tr_versicolor_avg_per_feature.reshape(4,1),tr_versicolor_cov)\n",
    "    h2_conf_virginica = ND_normal_dist(vl_data[:-1].reshape(4,1),tr_virginica_avg_per_feature.reshape(4,1),tr_virginica_cov)\n",
    "    \n",
    "    h2_prediction_class = np.argmax([h2_conf_setosa, h2_conf_versicolor, h2_conf_virginica])\n",
    "    \n",
    "    # add 1 if true_class != prediction_class \n",
    "    error_h2 += h2_prediction_class != vl_data[-1]\n",
    "    # hypothesis 2 END\n",
    "    \n",
    "print(error_h1/150)\n",
    "print(error_h2/150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.mlab as mlab\n",
    "f0 = 0\n",
    "f1 = 1\n",
    "min_x = min(iris.TrainData[:,f0])-0.1\n",
    "max_x = max(iris.TrainData[:,f0])+0.1\n",
    "min_y = min(iris.TrainData[:,f1])-0.1\n",
    "max_y = max(iris.TrainData[:,f1])+0.1\n",
    "cmap = plt.cm.magma\n",
    "y = np.linspace(min_x, max_x)\n",
    "x = np.linspace(min_y, max_y)\n",
    "\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "fig, ax = plt.subplots(2,2, figsize=(14, 14), dpi= 80, facecolor='w', edgecolor='k' )\n",
    "\n",
    "for i in range(2):\n",
    "    sc1, sc2, sc3 = ax[i,0].plot(\n",
    "        tr_setosa[:,f1],tr_setosa[:,f0],'o',\n",
    "        tr_versicolor[:,f1],tr_versicolor[:,f0],'o',\n",
    "        tr_virginica[:,f1],tr_virginica[:,f0],'o'\n",
    "    )\n",
    "\n",
    "fig.legend((sc1, sc2, sc3),\n",
    "           tuple(iris.set_names),\n",
    "           'upper center',\n",
    "           fontsize=16,\n",
    "           title='classes',\n",
    "          );\n",
    "\n",
    "fig.text(0.5, 0.04, Irisdataset.feature_names[f1], ha='center',fontsize=22)\n",
    "fig.text(0.04, 0.5, Irisdataset.feature_names[f0], va='center', rotation='vertical',fontsize=22)\n",
    "\n",
    "left  = 0.125\n",
    "right = 1\n",
    "bottom = 0.1\n",
    "top = 0.85\n",
    "wspace = 0.2\n",
    "hspace = 0.3\n",
    "\n",
    "fig.subplots_adjust(\n",
    "    left  = left,\n",
    "    right = right,\n",
    "    bottom =bottom,\n",
    "    top = top,\n",
    "    wspace = wspace,\n",
    "    hspace = hspace\n",
    ")\n",
    "\n",
    "# hypothesis 1\n",
    "var_set = tr_setosa_std_per_feature[[f0,f1]]\n",
    "avg_set = tr_setosa_avg_per_feature[[f0,f1]]\n",
    "Z_set = mlab.bivariate_normal(X, Y, var_set[1], var_set[0], avg_set[1], avg_set[0])\n",
    "\n",
    "var_ver = tr_versicolor_std_per_feature[[f0,f1]]\n",
    "avg_ver = tr_versicolor_avg_per_feature[[f0,f1]]\n",
    "Z_ver = mlab.bivariate_normal(X, Y, var_ver[1], var_ver[0], avg_ver[1], avg_ver[0])\n",
    "\n",
    "var_vir = tr_virginica_std_per_feature[[f0,f1]]\n",
    "avg_vir = tr_virginica_avg_per_feature[[f0,f1]]\n",
    "Z_vir = mlab.bivariate_normal(X, Y, var_vir[1], var_vir[0], avg_vir[1], avg_vir[0])\n",
    "Z = Z_set+Z_ver+Z_vir\n",
    "\n",
    "ax[0,0].contour(X, Y, Z_set,cmap = plt.cm.winter_r)\n",
    "ax[0,0].contour(X, Y, Z_ver,cmap = plt.cm.autumn)\n",
    "ax[0,0].contour(X, Y, Z_vir,cmap = plt.cm.summer_r)\n",
    "ax[0,0].set_title('Naive Base Hypothesis_1 classifiers')\n",
    "\n",
    "ax[0,1].axis('off')\n",
    "ax[0,1].imshow(Z, interpolation='bilinear', cmap=cmap,\n",
    "                origin='lower', extent=[min_x, max_x, min_y, max_y],\n",
    "                )\n",
    "\n",
    "ax[0,1].set_title('Hypothesis_1 Classifiers magnitude')\n",
    "\n",
    "# hypothesis 2\n",
    "cov_set = np.cov((tr_setosa[:,[f0,f1]]).T).flatten()\n",
    "cov_set[[0,-1]] = cov_set[[0,-1]]**(1/2)\n",
    "Z_set_cov = mlab.bivariate_normal(X, Y, cov_set[-1], cov_set[0], avg_set[1], avg_set[0],cov_set[1])\n",
    "\n",
    "cov_ver = np.cov((tr_versicolor[:,[f0,f1]]).T).flatten()**(1/2)\n",
    "cov_ver[[0,-1]] = cov_ver[[0,-1]]**(1/2)\n",
    "Z_ver_cov = mlab.bivariate_normal(X, Y, cov_ver[-1], cov_ver[0], avg_ver[1], avg_ver[0],cov_ver[1])\n",
    "\n",
    "cov_vir = np.cov((tr_virginica[:,[f0,f1]]).T).flatten()**(1/2)\n",
    "cov_vir[[0,-1]] = cov_vir[[0,-1]]**(1/2)\n",
    "Z_vir_cov = mlab.bivariate_normal(X, Y, cov_vir[-1], cov_vir[0], avg_vir[1], avg_vir[0],cov_vir[1])\n",
    "\n",
    "ax[1,0].contour(X, Y, Z_set_cov,cmap = plt.cm.winter_r)\n",
    "ax[1,0].contour(X, Y, Z_ver_cov,cmap = plt.cm.autumn)\n",
    "ax[1,0].contour(X, Y, Z_vir_cov,cmap = plt.cm.summer_r)\n",
    "\n",
    "\n",
    "Z_cov = Z_set_cov+Z_ver_cov+Z_vir_cov\n",
    "ax[1,1].axis('off')\n",
    "ax[1,1].imshow(Z_cov, interpolation='bilinear', cmap=cmap,\n",
    "                origin='lower', extent=[min_x, max_x, min_y, max_y],\n",
    "                )\n",
    "\n",
    "ax[1,0].set_title('Naive Base Hypothesis_2 classifiers')\n",
    "ax[1,1].set_title('Hypothesis_2 Classifiers magnitude')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
